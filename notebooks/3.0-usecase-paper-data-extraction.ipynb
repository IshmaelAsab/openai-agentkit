{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2740bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from pypdf import PdfReader\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class PaperData(BaseModel):\n",
    "    title: str = Field(description=\"The title of the paper\")\n",
    "    abstract_summary: List[str] = Field(description=\"One sentence summary of the abstract\")\n",
    "    intro_backgrounds_summary: str = Field(description=\"One sentence summary of the intro and backgrounds sections\")\n",
    "    methods_summary: str = Field(description=\"One sentence summary of the methods section\")\n",
    "    results_summary: str = Field(description=\"One sentence summary of the results section\")\n",
    "    discussion_summary: str = Field(description=\"One sentence summary of the discussion section\")\n",
    "    \n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    '''Loads text from a PDF file.'''\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # extracting text from page\n",
    "    text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    \n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1695146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-26 13:45:11--  https://arxiv.org/pdf/2510.26493\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.3.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2371045 (2,3M) [application/pdf]\n",
      "Saving to: ‘../assets/paper.pdf’\n",
      "\n",
      "../assets/paper.pdf 100%[===================>]   2,26M  --.-KB/s    in 0,07s   \n",
      "\n",
      "2025-11-26 13:45:11 (32,0 MB/s) - ‘../assets/paper.pdf’ saved [2371045/2371045]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ../assets/paper.pdf https://arxiv.org/pdf/2510.26493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfea3748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParsedResponse[PaperData](id='resp_081192adf42d638900692735ec66088197aa2970ed9be6f7b9', created_at=1764177388.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_081192adf42d638900692735ed864c8197936307c3784efc0b', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ParsedResponseOutputMessage[PaperData](id='msg_081192adf42d638900692735f236d08197a2c8038b2961bad1', content=[ParsedResponseOutputText[PaperData](annotations=[], text='{\"title\":\"Context Engineering 2.0: The Context of Context Engineering\",\"abstract_summary\":[\"The paper defines and situates context engineering as the systematic process of designing, organizing, and managing contextual information to reduce entropy between human intentions and machine understanding, traces its evolution over four eras from early HCI/context-aware systems to speculative superhuman intelligence, formalizes context and context engineering mathematically, and proposes design considerations for context collection, management, and usage while identifying open challenges and future directions.\"],\"intro_backgrounds_summary\":\"The introduction motivates context engineering by highlighting the impact of context on LLM and agent behavior, argues that context engineering predates modern LLM practices (rooted in ubiquitous computing and HCI), and frames the core task as compressing high-entropy human contexts into machine-understandable low-entropy representations.\",\"methods_summary\":\"The paper formalizes context with mathematical definitions (entities, Char, Context) and defines context engineering as a transformation CE:(C,T)→f_context composed from modular operations (ϕi), and it proposes a four-stage evolutionary characterization (Era 1.0–4.0) tied to machine intelligence levels.\",\"results_summary\":\"Rather than empirical experiments, the paper presents a conceptual and historical analysis: a comparison of Era 1.0 vs 2.0 practices, catalogs of context collection/storage/management/usage techniques (e.g., multimodal encoding, hierarchical memory, subagents, RAG, embeddings), representative system patterns and applications (CLI, deep research agents, BCIs), and a set of emerging engineering practices and trade-offs.\",\"discussion_summary\":\"The discussion highlights key challenges for lifelong and large-scale context engineering (storage bottlenecks, processing degradation, system instability, evaluation difficulty), argues for new architectures and a ‘‘semantic operating system’’ for context, and forecasts increasing machine responsibility in interpreting and constructing context as intelligence advances.\"}', type='output_text', logprobs=[], parsed=PaperData(title='Context Engineering 2.0: The Context of Context Engineering', abstract_summary=['The paper defines and situates context engineering as the systematic process of designing, organizing, and managing contextual information to reduce entropy between human intentions and machine understanding, traces its evolution over four eras from early HCI/context-aware systems to speculative superhuman intelligence, formalizes context and context engineering mathematically, and proposes design considerations for context collection, management, and usage while identifying open challenges and future directions.'], intro_backgrounds_summary='The introduction motivates context engineering by highlighting the impact of context on LLM and agent behavior, argues that context engineering predates modern LLM practices (rooted in ubiquitous computing and HCI), and frames the core task as compressing high-entropy human contexts into machine-understandable low-entropy representations.', methods_summary='The paper formalizes context with mathematical definitions (entities, Char, Context) and defines context engineering as a transformation CE:(C,T)→f_context composed from modular operations (ϕi), and it proposes a four-stage evolutionary characterization (Era 1.0–4.0) tied to machine intelligence levels.', results_summary='Rather than empirical experiments, the paper presents a conceptual and historical analysis: a comparison of Era 1.0 vs 2.0 practices, catalogs of context collection/storage/management/usage techniques (e.g., multimodal encoding, hierarchical memory, subagents, RAG, embeddings), representative system patterns and applications (CLI, deep research agents, BCIs), and a set of emerging engineering practices and trade-offs.', discussion_summary='The discussion highlights key challenges for lifelong and large-scale context engineering (storage bottlenecks, processing degradation, system instability, evaluation difficulty), argues for new architectures and a ‘‘semantic operating system’’ for context, and forecasts increasing machine responsibility in interpreting and constructing context as intelligence advances.'))], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatTextJSONSchemaConfig(name='PaperData', schema_={'properties': {'title': {'description': 'The title of the paper', 'title': 'Title', 'type': 'string'}, 'abstract_summary': {'description': 'One sentence summary of the abstract', 'items': {'type': 'string'}, 'title': 'Abstract Summary', 'type': 'array'}, 'intro_backgrounds_summary': {'description': 'One sentence summary of the intro and backgrounds sections', 'title': 'Intro Backgrounds Summary', 'type': 'string'}, 'methods_summary': {'description': 'One sentence summary of the methods section', 'title': 'Methods Summary', 'type': 'string'}, 'results_summary': {'description': 'One sentence summary of the results section', 'title': 'Results Summary', 'type': 'string'}, 'discussion_summary': {'description': 'One sentence summary of the discussion section', 'title': 'Discussion Summary', 'type': 'string'}}, 'required': ['title', 'abstract_summary', 'intro_backgrounds_summary', 'methods_summary', 'results_summary', 'discussion_summary'], 'title': 'PaperData', 'type': 'object', 'additionalProperties': False}, type='json_schema', description=None, strict=True), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=27742, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=708, output_tokens_details=OutputTokensDetails(reasoning_tokens=320), total_tokens=28450), user=None, billing={'payer': 'developer'}, store=True)\n"
     ]
    }
   ],
   "source": [
    "paper_raw_text = load_pdf_text(\"../assets/paper.pdf\")\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=f\"extract the information from the paper: {paper_raw_text}\",\n",
    "    text_format=PaperData\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b214a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaperData(title='Context Engineering 2.0: The Context of Context Engineering', abstract_summary=['The paper defines and situates context engineering as the systematic process of designing, organizing, and managing contextual information to reduce entropy between human intentions and machine understanding, traces its evolution over four eras from early HCI/context-aware systems to speculative superhuman intelligence, formalizes context and context engineering mathematically, and proposes design considerations for context collection, management, and usage while identifying open challenges and future directions.'], intro_backgrounds_summary='The introduction motivates context engineering by highlighting the impact of context on LLM and agent behavior, argues that context engineering predates modern LLM practices (rooted in ubiquitous computing and HCI), and frames the core task as compressing high-entropy human contexts into machine-understandable low-entropy representations.', methods_summary='The paper formalizes context with mathematical definitions (entities, Char, Context) and defines context engineering as a transformation CE:(C,T)→f_context composed from modular operations (ϕi), and it proposes a four-stage evolutionary characterization (Era 1.0–4.0) tied to machine intelligence levels.', results_summary='Rather than empirical experiments, the paper presents a conceptual and historical analysis: a comparison of Era 1.0 vs 2.0 practices, catalogs of context collection/storage/management/usage techniques (e.g., multimodal encoding, hierarchical memory, subagents, RAG, embeddings), representative system patterns and applications (CLI, deep research agents, BCIs), and a set of emerging engineering practices and trade-offs.', discussion_summary='The discussion highlights key challenges for lifelong and large-scale context engineering (storage bottlenecks, processing degradation, system instability, evaluation difficulty), argues for new architectures and a ‘‘semantic operating system’’ for context, and forecasts increasing machine responsibility in interpreting and constructing context as intelligence advances.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0d9ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Context Engineering 2.0: The Context of Context Engineering\n",
       "\n",
       "## Abstract Summary\n",
       "The paper defines and situates context engineering as the systematic process of designing, organizing, and managing contextual information to reduce entropy between human intentions and machine understanding, traces its evolution over four eras from early HCI/context-aware systems to speculative superhuman intelligence, formalizes context and context engineering mathematically, and proposes design considerations for context collection, management, and usage while identifying open challenges and future directions.\n",
       "\n",
       "## Introduction & Backgrounds Summary\n",
       "The introduction motivates context engineering by highlighting the impact of context on LLM and agent behavior, argues that context engineering predates modern LLM practices (rooted in ubiquitous computing and HCI), and frames the core task as compressing high-entropy human contexts into machine-understandable low-entropy representations.\n",
       "\n",
       "## Methods Summary\n",
       "The paper formalizes context with mathematical definitions (entities, Char, Context) and defines context engineering as a transformation CE:(C,T)→f_context composed from modular operations (ϕi), and it proposes a four-stage evolutionary characterization (Era 1.0–4.0) tied to machine intelligence levels.\n",
       "\n",
       "## Results Summary\n",
       "Rather than empirical experiments, the paper presents a conceptual and historical analysis: a comparison of Era 1.0 vs 2.0 practices, catalogs of context collection/storage/management/usage techniques (e.g., multimodal encoding, hierarchical memory, subagents, RAG, embeddings), representative system patterns and applications (CLI, deep research agents, BCIs), and a set of emerging engineering practices and trade-offs.\n",
       "\n",
       "## Discussion Summary\n",
       "The discussion highlights key challenges for lifelong and large-scale context engineering (storage bottlenecks, processing degradation, system instability, evaluation difficulty), argues for new architectures and a ‘‘semantic operating system’’ for context, and forecasts increasing machine responsibility in interpreting and constructing context as intelligence advances.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "md = f\"\"\"\n",
    "# {response.output_parsed.title}\n",
    "\n",
    "## Abstract Summary\n",
    "{'; '.join(response.output_parsed.abstract_summary)}\n",
    "\n",
    "## Introduction & Backgrounds Summary\n",
    "{response.output_parsed.intro_backgrounds_summary}\n",
    "\n",
    "## Methods Summary\n",
    "{response.output_parsed.methods_summary}\n",
    "\n",
    "## Results Summary\n",
    "{response.output_parsed.results_summary}\n",
    "\n",
    "## Discussion Summary\n",
    "{response.output_parsed.discussion_summary}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30883e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new entry to 'papers_database.csv' for title='Context Engineering 2.0: The Context of Context Engineering'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def upsert_paper_to_csv(paper_data, csv_path=\"papers_database.csv\", key_column=\"title\"):\n",
    "    \"\"\"\n",
    "    Inserts or updates a paper's data in a CSV file database. If the file does not exist,\n",
    "    it creates it. If the file exists, it updates the entry if the key exists, or appends \n",
    "    the new paper if not.\n",
    "\n",
    "    Parameters:\n",
    "        paper_data: an object with attributes corresponding to extracted fields\n",
    "        csv_path: output file path for CSV\n",
    "        key_column: column name to use as unique identifier (default: \"title\")\n",
    "    \"\"\"\n",
    "    # Prepare data dictionary, flattening abstract_summary if it's a list\n",
    "    paper_dict = {\n",
    "        \"title\": getattr(paper_data, \"title\", \"\"),\n",
    "        \"abstract_summary\": \"; \".join(paper_data.abstract_summary) if isinstance(getattr(paper_data, \"abstract_summary\", \"\"), list) else getattr(paper_data, \"abstract_summary\", \"\"),\n",
    "        \"intro_backgrounds_summary\": getattr(paper_data, \"intro_backgrounds_summary\", \"\"),\n",
    "        \"methods_summary\": getattr(paper_data, \"methods_summary\", \"\"),\n",
    "        \"results_summary\": getattr(paper_data, \"results_summary\", \"\"),\n",
    "        \"discussion_summary\": getattr(paper_data, \"discussion_summary\", \"\"),\n",
    "    }\n",
    "\n",
    "    # If CSV exists, load and update if key matches; otherwise create new\n",
    "    if os.path.isfile(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Check if key (e.g., title) already in db\n",
    "        if paper_dict[key_column] in df[key_column].values:\n",
    "            # Update existing entry\n",
    "            df.loc[df[key_column] == paper_dict[key_column], list(paper_dict.keys())] = list(paper_dict.values())\n",
    "            updated = True\n",
    "        else:\n",
    "            # Append if not found\n",
    "            df = pd.concat([df, pd.DataFrame([paper_dict])], ignore_index=True)\n",
    "            updated = False\n",
    "    else:\n",
    "        # Create new dataframe\n",
    "        df = pd.DataFrame([paper_dict])\n",
    "        updated = False\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    if updated:\n",
    "        print(f\"Updated entry in '{csv_path}' for {key_column}='{paper_dict[key_column]}'\")\n",
    "    else:\n",
    "        print(f\"Added new entry to '{csv_path}' for {key_column}='{paper_dict[key_column]}'\")\n",
    "\n",
    "# Example usage:\n",
    "upsert_paper_to_csv(response.output_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd483d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-25 16:25:46--  https://arxiv.org/pdf/1301.3781\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.195.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 228716 (223K) [application/pdf]\n",
      "Saving to: ‘paper3.pdf’\n",
      "\n",
      "paper3.pdf          100%[===================>] 223,36K  --.-KB/s    in 0,02s   \n",
      "\n",
      "2025-11-25 16:25:46 (9,24 MB/s) - ‘paper3.pdf’ saved [228716/228716]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ../assets/paper3.pdf \"https://arxiv.org/pdf/1301.3781\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb718b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Efﬁcient Estimation of Word Representations in Vector Space' abstract_summary=['The paper introduces two efficient model architectures (Continuous Bag-of-Words — CBOW — and Continuous Skip-gram) for learning continuous word vector representations from very large datasets, achieving state-of-the-art syntactic and semantic analogy performance with much lower computational cost and practical training times on billions of words.'] intro_backgrounds_summary='The authors motivate moving beyond atomic word representations to distributed continuous vectors (learned by neural methods) to capture multiple degrees of similarity and linear regularities (e.g., vector arithmetic like king - man + woman = queen), and set the goal of learning high-quality vectors from massive corpora and large vocabularies.' methods_summary='They propose two log-linear architectures—CBOW (predict current word from averaged context vectors) and Skip-gram (predict surrounding words given the current word)—use hierarchical softmax (Huffman tree) and SGD (with Adagrad) to reduce complexity, analyze per-example computational costs, and implement large-scale parallel training in the DistBelief framework.' results_summary='Experimentally, Skip-gram yields the best semantic analogy performance and CBOW is very efficient on syntactic tasks; both outperform or match prior NNLM/RNNLM vectors on a comprehensive semantic–syntactic analogy test and, when combined with RNNLMs, improve performance on the Microsoft Sentence Completion Challenge; models train in hours–days on multi-billion-word corpora.' discussion_summary='The paper concludes that very simple, computationally efficient architectures can produce high-quality, scalable word vectors suitable for large corpora (and practical NLP use), releases code and pretrained vectors (word2vec), and outlines promising applications and directions for further scaling and improvements.'\n",
      "Added new entry to 'papers_database.csv' for title='Efﬁcient Estimation of Word Representations in Vector Space'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PaperData(title='Efﬁcient Estimation of Word Representations in Vector Space', abstract_summary=['The paper introduces two efficient model architectures (Continuous Bag-of-Words — CBOW — and Continuous Skip-gram) for learning continuous word vector representations from very large datasets, achieving state-of-the-art syntactic and semantic analogy performance with much lower computational cost and practical training times on billions of words.'], intro_backgrounds_summary='The authors motivate moving beyond atomic word representations to distributed continuous vectors (learned by neural methods) to capture multiple degrees of similarity and linear regularities (e.g., vector arithmetic like king - man + woman = queen), and set the goal of learning high-quality vectors from massive corpora and large vocabularies.', methods_summary='They propose two log-linear architectures—CBOW (predict current word from averaged context vectors) and Skip-gram (predict surrounding words given the current word)—use hierarchical softmax (Huffman tree) and SGD (with Adagrad) to reduce complexity, analyze per-example computational costs, and implement large-scale parallel training in the DistBelief framework.', results_summary='Experimentally, Skip-gram yields the best semantic analogy performance and CBOW is very efficient on syntactic tasks; both outperform or match prior NNLM/RNNLM vectors on a comprehensive semantic–syntactic analogy test and, when combined with RNNLMs, improve performance on the Microsoft Sentence Completion Challenge; models train in hours–days on multi-billion-word corpora.', discussion_summary='The paper concludes that very simple, computationally efficient architectures can produce high-quality, scalable word vectors suitable for large corpora (and practical NLP use), releases code and pretrained vectors (word2vec), and outlines promising applications and directions for further scaling and improvements.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_paper_data(paper_path: str):\n",
    "    paper_raw_text = load_pdf_text(paper_path)\n",
    "\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=f\"extract the information from the paper: {paper_raw_text}\",\n",
    "        text_format=PaperData\n",
    "    )\n",
    "    \n",
    "    output_parsed = response.output_parsed\n",
    "\n",
    "    print(output_parsed)\n",
    "    \n",
    "    upsert_paper_to_csv(output_parsed)\n",
    "    \n",
    "    return output_parsed\n",
    "\n",
    "extract_paper_data(\"../assets/paper3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49ec0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_summary</th>\n",
       "      <th>intro_backgrounds_summary</th>\n",
       "      <th>methods_summary</th>\n",
       "      <th>results_summary</th>\n",
       "      <th>discussion_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Context Engineering 2.0: The Context of Contex...</td>\n",
       "      <td>The paper defines and situates context enginee...</td>\n",
       "      <td>The introduction motivates context engineering...</td>\n",
       "      <td>The paper formalizes context with mathematical...</td>\n",
       "      <td>Rather than empirical experiments, the paper p...</td>\n",
       "      <td>The discussion highlights key challenges for l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efﬁcient Estimation of Word Representations in...</td>\n",
       "      <td>The paper introduces two efficient model archi...</td>\n",
       "      <td>The authors motivate moving beyond atomic word...</td>\n",
       "      <td>They propose two log-linear architectures—CBOW...</td>\n",
       "      <td>Experimentally, Skip-gram yields the best sema...</td>\n",
       "      <td>The paper concludes that very simple, computat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Context Engineering 2.0: The Context of Contex...   \n",
       "1  Efﬁcient Estimation of Word Representations in...   \n",
       "\n",
       "                                    abstract_summary  \\\n",
       "0  The paper defines and situates context enginee...   \n",
       "1  The paper introduces two efficient model archi...   \n",
       "\n",
       "                           intro_backgrounds_summary  \\\n",
       "0  The introduction motivates context engineering...   \n",
       "1  The authors motivate moving beyond atomic word...   \n",
       "\n",
       "                                     methods_summary  \\\n",
       "0  The paper formalizes context with mathematical...   \n",
       "1  They propose two log-linear architectures—CBOW...   \n",
       "\n",
       "                                     results_summary  \\\n",
       "0  Rather than empirical experiments, the paper p...   \n",
       "1  Experimentally, Skip-gram yields the best sema...   \n",
       "\n",
       "                                  discussion_summary  \n",
       "0  The discussion highlights key challenges for l...  \n",
       "1  The paper concludes that very simple, computat...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../assets/papers_database.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "774dcfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'name': 'extract_paper_data',\n",
       " 'description': 'Extracts data from a paper',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'paper_path': {'type': 'string',\n",
       "    'description': 'The path to the paper'}},\n",
       "  'required': ['paper_path']}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"extract_paper_data\",\n",
    "    \"description\": \"Extracts data from a paper\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"paper_path\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The path to the paper\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"paper_path\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "tool_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dc74f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-26 17:23:32--  https://arxiv.org/pdf/2412.14161\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.195.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2169754 (2,1M) [application/pdf]\n",
      "Saving to: ‘./test_agent_paper.pdf’\n",
      "\n",
      "./test_agent_paper. 100%[===================>]   2,07M  --.-KB/s    in 0,06s   \n",
      "\n",
      "2025-11-26 17:23:33 (33,7 MB/s) - ‘./test_agent_paper.pdf’ saved [2169754/2169754]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ./test_agent_paper.pdf \"https://arxiv.org/pdf/2412.14161\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b02dbfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    instructions=\"You are an extraction agent, users will give you paths to files for pdfs\\\n",
    "                  and you will extract the data from the paper using the:\\\n",
    "                  extract_paper_data function.\",\n",
    "    tools=[tool_schema],\n",
    "    input=\"Extract data from this paper: ./test_agent_paper.pdf\"\n",
    ")\n",
    "\n",
    "response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "237dd3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_0cc5319de7b42f93006927383a59588195ab5045551da875a4', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{\"paper_path\":\"./test_agent_paper.pdf\"}', call_id='call_lBrW8pLIfaO2o8FOCel95PtU', name='extract_paper_data', type='function_call', id='fc_0cc5319de7b42f93006927383af9dc8195944fb0fa775c0ac9', status='completed')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a8add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractPaperData(BaseModel):\n",
    "    paper_path: str = Field(description=\"The path to the paper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8f63be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'paper_path': {'description': 'The path to the paper',\n",
       "   'title': 'Paper Path',\n",
       "   'type': 'string'}},\n",
       " 'required': ['paper_path'],\n",
       " 'title': 'ExtractPaperData',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_paper_data_schema = ExtractPaperData.model_json_schema()\n",
    "\n",
    "extract_paper_data_schema\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
