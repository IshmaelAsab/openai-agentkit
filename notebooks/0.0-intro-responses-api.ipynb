{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f64ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fec2ea",
   "metadata": {},
   "source": [
    "# Basic Text Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aaaee84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intelligence will evolve into a globally distributed, hybrid ecosystem of human, artificial, and collective minds that amplifies creativity and problem-solving while forcing new social, ethical, and governance frameworks to manage its risks and distribution.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model='gpt-5-mini',\n",
    "    input='Write a one sentence prediction for the future of intelligence.'\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675bd20",
   "metadata": {},
   "source": [
    "# Analyse Images and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540783f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A starry night landscape with a rugged, snow-capped mountain range. The foreground has patches of snow on dark slopes, and faint clouds sit low under a clear, starlit sky.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "             {\n",
    "                 \"type\": \"input_text\",\n",
    "                 \"text\": \"What is in this image?\"\n",
    "             },\n",
    "             {\n",
    "                 \"type\": \"input_image\",\n",
    "                 \"image_url\": \"https://images.unsplash.com/photo-1464983953574-0892a716854b\",\n",
    "              }\n",
    "             ]\n",
    "         }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab8511",
   "metadata": {},
   "source": [
    "# Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f55a850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasks=['John: review the Q2 budget proposal', 'Set up a kickoff meeting for the new project', 'Anna: update and submit the marketing plan', 'Update status before our next check-in'] deadlines=['end of this week', 'next Monday', 'April 15th', 'before our next check-in']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Tasks(BaseModel):\n",
    "    tasks: list[str]\n",
    "    deadlines: list[str]\n",
    "\n",
    "meeting_notes = \"\"\"\n",
    "Yesterday we discussed several action items:\n",
    "• John needs to review the Q2 budget proposal before the end of this week.\n",
    "• Let's set up a kickoff meeting for the new project—how about next Monday?\n",
    "• Anna will update and submit the marketing plan by April 15th.\n",
    "Please make sure to update the status before our next check-in.\n",
    "\"\"\"\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You Extract tasks and their deadlines from raw inputs.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": meeting_notes\n",
    "         }\n",
    "    ],\n",
    "    text_format=Tasks,\n",
    ")\n",
    "\n",
    "print(response.output_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82179ecd",
   "metadata": {},
   "source": [
    "# Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "531044ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final input:\n",
      "[{'role': 'user', 'content': 'Can you read and summarize the file at ../assets/paper3.pdf?'}, ResponseReasoningItem(id='rs_006bc7ff483635a100695cf77bc79c8197b0fec4105bde9a50', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseFunctionToolCall(arguments='{\"file_path\":\"../assets/paper3.pdf\"}', call_id='call_cTsPsE9ORthI48yuXAdpPWB4', name='load_pdf_text', type='function_call', id='fc_006bc7ff483635a100695cf77ceb58819799344c8af232a3a9', status='completed'), {'type': 'function_call_output', 'call_id': 'call_cTsPsE9ORthI48yuXAdpPWB4', 'output': '{\"pdf_text\": \"Ef\\\\ufb01cient Estimation of Word Representations in\\\\nVector Space\\\\nTomas Mikolov\\\\nGoogle Inc., Mountain View, CA\\\\ntmikolov@google.com\\\\nKai Chen\\\\nGoogle Inc., Mountain View, CA\\\\nkaichen@google.com\\\\nGreg Corrado\\\\nGoogle Inc., Mountain View, CA\\\\ngcorrado@google.com\\\\nJeffrey Dean\\\\nGoogle Inc., Mountain View, CA\\\\njeff@google.com\\\\nAbstract\\\\nWe propose two novel model architectures for computing continuous vector repre-\\\\nsentations of words from very large data sets. The quality of these representations\\\\nis measured in a word similarity task, and the results are compared to the previ-\\\\nously best performing techniques based on different types of neural networks. We\\\\nobserve large improvements in accuracy at much lower computational cost, i.e. it\\\\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\\\\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\\\\nmance on our test set for measuring syntactic and semantic word similarities.\\\\n1 Introduction\\\\nMany current NLP systems and techniques treat words as atomic units - there is no notion of similar-\\\\nity between words, as these are represented as indices in a vocabulary. This choice has several good\\\\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\\\\ndata outperform complex systems trained on less data. An example is the popular N-gram model\\\\nused for statistical language modeling - today, it is possible to train N-grams on virtually all available\\\\ndata (trillions of words [3]).\\\\nHowever, the simple techniques are at their limits in many tasks. For example, the amount of\\\\nrelevant in-domain data for automatic speech recognition is limited - the performance is usually\\\\ndominated by the size of high quality transcribed speech data (often just millions of words). In\\\\nmachine translation, the existing corpora for many languages contain only a few billions of words\\\\nor less. Thus, there are situations where simple scaling up of the basic techniques will not result in\\\\nany signi\\\\ufb01cant progress, and we have to focus on more advanced techniques.\\\\nWith progress of machine learning techniques in recent years, it has become possible to train more\\\\ncomplex models on much larger data set, and they typically outperform the simple models. Probably\\\\nthe most successful concept is to use distributed representations of words [10]. For example, neural\\\\nnetwork based language models signi\\\\ufb01cantly outperform N-gram models [1, 27, 17].\\\\n1.1 Goals of the Paper\\\\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word\\\\nvectors from huge data sets with billions of words, and with millions of words in the vocabulary. As\\\\nfar as we know, none of the previously proposed architectures has been successfully trained on more\\\\n1\\\\narXiv:1301.3781v3  [cs.CL]  7 Sep 2013\\\\n\\\\nthan a few hundred of millions of words, with a modest dimensionality of the word vectors between\\\\n50 - 100.\\\\nWe use recently proposed techniques for measuring the quality of the resulting vector representa-\\\\ntions, with the expectation that not only will similar words tend to be close to each other, but that\\\\nwords can have multiple degrees of similarity [20]. This has been observed earlier in the context\\\\nof in\\\\ufb02ectional languages - for example, nouns can have multiple word endings, and if we search for\\\\nsimilar words in a subspace of the original vector space, it is possible to \\\\ufb01nd words that have similar\\\\nendings [13, 14].\\\\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple\\\\nsyntactic regularities. Using a word offset technique where simple algebraic operations are per-\\\\nformed on the word vectors, it was shown for example that vector(\\\\u201dKing\\\\u201d) - vector(\\\\u201dMan\\\\u201d) + vec-\\\\ntor(\\\\u201dWoman\\\\u201d) results in a vector that is closest to the vector representation of the wordQueen [20].\\\\nIn this paper, we try to maximize accuracy of these vector operations by developing new model\\\\narchitectures that preserve the linear regularities among words. We design a new comprehensive test\\\\nset for measuring both syntactic and semantic regularities 1, and show that many such regularities\\\\ncan be learned with high accuracy. Moreover, we discuss how training time and accuracy depends\\\\non the dimensionality of the word vectors and on the amount of the training data.\\\\n1.2 Previous Work\\\\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\\\\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\\\\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to\\\\nlearn jointly the word vector representation and a statistical language model. This work has been\\\\nfollowed by many others.\\\\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are\\\\n\\\\ufb01rst learned using neural network with a single hidden layer. The word vectors are then used to train\\\\nthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this\\\\nwork, we directly extend this architecture, and focus just on the \\\\ufb01rst step where the word vectors are\\\\nlearned using a simple model.\\\\nIt was later shown that the word vectors can be used to signi\\\\ufb01cantly improve and simplify many\\\\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\\\\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word\\\\nvectors were made available for future research and comparison2. However, as far as we know, these\\\\narchitectures were signi\\\\ufb01cantly more computationally expensive for training than the one proposed\\\\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\\\\nare used [23].\\\\n2 Model Architectures\\\\nMany different types of models were proposed for estimating continuous representations of words,\\\\nincluding the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\\\\nIn this paper, we focus on distributed representations of words learned by neural networks, as it was\\\\npreviously shown that they perform signi\\\\ufb01cantly better than LSA for preserving linear regularities\\\\namong words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\\\\nSimilar to [18], to compare different model architectures we de\\\\ufb01ne \\\\ufb01rst the computational complex-\\\\nity of a model as the number of parameters that need to be accessed to fully train the model. Next,\\\\nwe will try to maximize the accuracy, while minimizing the computational complexity.\\\\n1The test set is available at www.fit.vutbr.cz/\\\\u02dcimikolov/rnnlm/word-test.v1.txt\\\\n2http://ronan.collobert.com/senna/\\\\nhttp://metaoptimize.com/projects/wordreprs/\\\\nhttp://www.fit.vutbr.cz/\\\\u02dcimikolov/rnnlm/\\\\nhttp://ai.stanford.edu/\\\\u02dcehhuang/\\\\n2\\\\n\\\\nFor all the following models, the training complexity is proportional to\\\\nO = E \\\\u00d7 T \\\\u00d7 Q, (1)\\\\nwhere E is number of the training epochs, T is the number of the words in the training set and Q is\\\\nde\\\\ufb01ned further for each model architecture. Common choice is E = 3\\\\u2212 50 and T up to one billion.\\\\nAll models are trained using stochastic gradient descent and backpropagation [26].\\\\n2.1 Feedforward Neural Net Language Model (NNLM)\\\\nThe probabilistic feedforward neural network language model has been proposed in [1]. It consists\\\\nof input, projection, hidden and output layers. At the input layer, N previous words are encoded\\\\nusing 1-of- V coding, where V is size of the vocabulary. The input layer is then projected to a\\\\nprojection layer P that has dimensionality N \\\\u00d7 D, using a shared projection matrix. As only N\\\\ninputs are active at any given time, composition of the projection layer is a relatively cheap operation.\\\\nThe NNLM architecture becomes complex for computation between the projection and the hidden\\\\nlayer, as values in the projection layer are dense. For a common choice of N = 10, the size of the\\\\nprojection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000\\\\nunits. Moreover, the hidden layer is used to compute probability distribution over all the words in the\\\\nvocabulary, resulting in an output layer with dimensionalityV . Thus, the computational complexity\\\\nper each training example is\\\\nQ = N \\\\u00d7 D + N \\\\u00d7 D \\\\u00d7 H + H \\\\u00d7 V, (2)\\\\nwhere the dominating term is H \\\\u00d7 V . However, several practical solutions were proposed for\\\\navoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized\\\\nmodels completely by using models that are not normalized during training [4, 9]. With binary tree\\\\nrepresentations of the vocabulary, the number of output units that need to be evaluated can go down\\\\nto around log2(V ). Thus, most of the complexity is caused by the term N \\\\u00d7 D \\\\u00d7 H.\\\\nIn our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary\\\\ntree. This follows previous observations that the frequency of words works well for obtaining classes\\\\nin neural net language models [16]. Huffman trees assign short binary codes to frequent words, and\\\\nthis further reduces the number of output units that need to be evaluated: while balanced binary tree\\\\nwould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires\\\\nonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one million\\\\nwords, this results in about two times speedup in evaluation. While this is not crucial speedup for\\\\nneural network LMs as the computational bottleneck is in theN \\\\u00d7D\\\\u00d7H term, we will later propose\\\\narchitectures that do not have hidden layers and thus depend heavily on the ef\\\\ufb01ciency of the softmax\\\\nnormalization.\\\\n2.2 Recurrent Neural Net Language Model (RNNLM)\\\\nRecurrent neural network based language model has been proposed to overcome certain limitations\\\\nof the feedforward NNLM, such as the need to specify the context length (the order of the modelN),\\\\nand because theoretically RNNs can ef\\\\ufb01ciently represent more complex patterns than the shallow\\\\nneural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and\\\\noutput layer. What is special for this type of model is the recurrent matrix that connects hidden\\\\nlayer to itself, using time-delayed connections. This allows the recurrent model to form some kind\\\\nof short term memory, as information from the past can be represented by the hidden layer state that\\\\ngets updated based on the current input and the state of the hidden layer in the previous time step.\\\\nThe complexity per training example of the RNN model is\\\\nQ = H \\\\u00d7 H + H \\\\u00d7 V, (3)\\\\nwhere the word representations D have the same dimensionality as the hidden layer H. Again, the\\\\nterm H \\\\u00d7 V can be ef\\\\ufb01ciently reduced to H \\\\u00d7 log2(V ) by using hierarchical softmax. Most of the\\\\ncomplexity then comes from H \\\\u00d7 H.\\\\n3\\\\n\\\\n2.3 Parallel Training of Neural Networks\\\\nTo train models on huge data sets, we have implemented several models on top of a large-scale\\\\ndistributed framework called DistBelief [6], including the feedforward NNLM and the new models\\\\nproposed in this paper. The framework allows us to run multiple replicas of the same model in\\\\nparallel, and each replica synchronizes its gradient updates through a centralized server that keeps\\\\nall the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with\\\\nan adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use\\\\none hundred or more model replicas, each using many CPU cores at different machines in a data\\\\ncenter.\\\\n3 New Log-linear Models\\\\nIn this section, we propose two new model architectures for learning distributed representations\\\\nof words that try to minimize computational complexity. The main observation from the previous\\\\nsection was that most of the complexity is caused by the non-linear hidden layer in the model. While\\\\nthis is what makes neural networks so attractive, we decided to explore simpler models that might\\\\nnot be able to represent the data as precisely as neural networks, but can possibly be trained on much\\\\nmore data ef\\\\ufb01ciently.\\\\nThe new architectures directly follow those proposed in our earlier work [13, 14], where it was\\\\nfound that neural network language model can be successfully trained in two steps: \\\\ufb01rst, continuous\\\\nword vectors are learned using simple model, and then the N-gram NNLM is trained on top of these\\\\ndistributed representations of words. While there has been later substantial amount of work that\\\\nfocuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.\\\\nNote that related models have been proposed also much earlier [26, 8].\\\\n3.1 Continuous Bag-of-Words Model\\\\nThe \\\\ufb01rst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden\\\\nlayer is removed and the projection layer is shared for all words (not just the projection matrix);\\\\nthus, all words get projected into the same position (their vectors are averaged). We call this archi-\\\\ntecture a bag-of-words model as the order of words in the history does not in\\\\ufb02uence the projection.\\\\nFurthermore, we also use words from the future; we have obtained the best performance on the task\\\\nintroduced in the next section by building a log-linear classi\\\\ufb01er with four future and four history\\\\nwords at the input, where the training criterion is to correctly classify the current (middle) word.\\\\nTraining complexity is then\\\\nQ = N \\\\u00d7 D + D \\\\u00d7 log2(V ). (4)\\\\nWe denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous\\\\ndistributed representation of the context. The model architecture is shown at Figure 1. Note that the\\\\nweight matrix between the input and the projection layer is shared for all word positions in the same\\\\nway as in the NNLM.\\\\n3.2 Continuous Skip-gram Model\\\\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the\\\\ncontext, it tries to maximize classi\\\\ufb01cation of a word based on another word in the same sentence.\\\\nMore precisely, we use each current word as an input to a log-linear classi\\\\ufb01er with continuous\\\\nprojection layer, and predict words within a certain range before and after the current word. We\\\\nfound that increasing the range improves quality of the resulting word vectors, but it also increases\\\\nthe computational complexity. Since the more distant words are usually less related to the current\\\\nword than those close to it, we give less weight to the distant words by sampling less from those\\\\nwords in our training examples.\\\\nThe training complexity of this architecture is proportional to\\\\nQ = C \\\\u00d7 (D + D \\\\u00d7 log2(V )), (5)\\\\nwhere C is the maximum distance of the words. Thus, if we choose C = 5, for each training word\\\\nwe will select randomly a number R in range < 1; C >, and then use R words from history and\\\\n4\\\\n\\\\nw(t-2)\\\\nw(t+1)\\\\nw(t-1)\\\\nw(t+2)\\\\nw(t)\\\\nSUM\\\\n       INPUT         PROJECTION         OUTPUT\\\\nw(t)\\\\n          INPUT         PROJECTION      OUTPUT\\\\nw(t-2)\\\\nw(t-1)\\\\nw(t+1)\\\\nw(t+2)\\\\n                   CBOW                                                   Skip-gram\\\\nFigure 1: New model architectures. The CBOW architecture predicts the current word based on the\\\\ncontext, and the Skip-gram predicts surrounding words given the current word.\\\\nR words from the future of the current word as correct labels. This will require us to do R \\\\u00d7 2\\\\nword classi\\\\ufb01cations, with the current word as input, and each of the R + R words as output. In the\\\\nfollowing experiments, we use C = 10.\\\\n4 Results\\\\nTo compare the quality of different versions of word vectors, previous papers typically use a table\\\\nshowing example words and their most similar words, and understand them intuitively. Although\\\\nit is easy to show that word France is similar to Italy and perhaps some other countries, it is much\\\\nmore challenging when subjecting those vectors in a more complex similarity task, as follows. We\\\\nfollow previous observation that there can be many different types of similarities between words, for\\\\nexample, word big is similar to bigger in the same sense that small is similar to smaller. Example\\\\nof another type of relationship can be word pairs big - biggest and small - smallest [20]. We further\\\\ndenote two pairs of words with the same relationship as a question, as we can ask: \\\\u201dWhat is the\\\\nword that is similar to small in the same sense as biggest is similar to big?\\\\u201d\\\\nSomewhat surprisingly, these questions can be answered by performing simple algebraic operations\\\\nwith the vector representation of words. To \\\\ufb01nd a word that is similar to small in the same sense as\\\\nbiggest is similar to big, we can simply compute vectorX = vector(\\\\u201dbiggest\\\\u201d) \\\\u2212vector(\\\\u201dbig\\\\u201d) +\\\\nvector(\\\\u201dsmall\\\\u201d). Then, we search in the vector space for the word closest toX measured by cosine\\\\ndistance, and use it as the answer to the question (we discard the input question words during this\\\\nsearch). When the word vectors are well trained, it is possible to \\\\ufb01nd the correct answer (word\\\\nsmallest) using this method.\\\\nFinally, we found that when we train high dimensional word vectors on a large amount of data, the\\\\nresulting vectors can be used to answer very subtle semantic relationships between words, such as\\\\na city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\\\\nwith such semantic relationships could be used to improve many existing NLP applications, such\\\\nas machine translation, information retrieval and question answering systems, and may enable other\\\\nfuture applications yet to be invented.\\\\n5\\\\n\\\\nTable 1: Examples of \\\\ufb01ve types of semantic and nine types of syntactic questions in the Semantic-\\\\nSyntactic Word Relationship test set.\\\\nType of relationship Word Pair 1 Word Pair 2\\\\nCommon capital city Athens Greece Oslo Norway\\\\nAll capital cities Astana Kazakhstan Harare Zimbabwe\\\\nCurrency Angola kwanza Iran rial\\\\nCity-in-state Chicago Illinois Stockton California\\\\nMan-Woman brother sister grandson granddaughter\\\\nAdjective to adverb apparent apparently rapid rapidly\\\\nOpposite possibly impossibly ethical unethical\\\\nComparative great greater tough tougher\\\\nSuperlative easy easiest lucky luckiest\\\\nPresent Participle think thinking read reading\\\\nNationality adjective Switzerland Swiss Cambodia Cambodian\\\\nPast tense walking walked swimming swam\\\\nPlural nouns mouse mice dollar dollars\\\\nPlural verbs work works speak speaks\\\\n4.1 Task Description\\\\nTo measure quality of the word vectors, we de\\\\ufb01ne a comprehensive test set that contains \\\\ufb01ve types\\\\nof semantic questions, and nine types of syntactic questions. Two examples from each category are\\\\nshown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions\\\\nin each category were created in two steps: \\\\ufb01rst, a list of similar word pairs was created manually.\\\\nThen, a large list of questions is formed by connecting two word pairs. For example, we made a\\\\nlist of 68 large American cities and the states they belong to, and formed about 2.5K questions by\\\\npicking two word pairs at random. We have included in our test set only single token words, thus\\\\nmulti-word entities are not present (such as New York).\\\\nWe evaluate the overall accuracy for all question types, and for each question type separately (se-\\\\nmantic, syntactic). Question is assumed to be correctly answered only if the closest word to the\\\\nvector computed using the above method is exactly the same as the correct word in the question;\\\\nsynonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely\\\\nto be impossible, as the current models do not have any input information about word morphology.\\\\nHowever, we believe that usefulness of the word vectors for certain applications should be positively\\\\ncorrelated with this accuracy metric. Further progress can be achieved by incorporating information\\\\nabout structure of words, especially for the syntactic questions.\\\\n4.2 Maximization of Accuracy\\\\nWe have used a Google News corpus for training the word vectors. This corpus contains about\\\\n6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we\\\\nare facing time constrained optimization problem, as it can be expected that both using more data\\\\nand higher dimensional word vectors will improve the accuracy. To estimate the best choice of\\\\nmodel architecture for obtaining as good as possible results quickly, we have \\\\ufb01rst evaluated models\\\\ntrained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.\\\\nThe results using the CBOW architecture with different choice of word vector dimensionality and\\\\nincreasing amount of the training data are shown in Table 2.\\\\nIt can be seen that after some point, adding more dimensions or adding more training data provides\\\\ndiminishing improvements. So, we have to increase both vector dimensionality and the amount\\\\nof the training data together. While this observation might seem trivial, it must be noted that it is\\\\ncurrently popular to train word vectors on relatively large amounts of data, but with insuf\\\\ufb01cient size\\\\n6\\\\n\\\\nTable 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word\\\\nvectors from the CBOW architecture with limited vocabulary. Only questions containing words from\\\\nthe most frequent 30k words are used.\\\\nDimensionality / Training words 24M 49M 98M 196M 391M 783M\\\\n50 13.4 15.7 18.6 19.1 22.5 23.2\\\\n100 19.4 23.1 27.8 28.7 33.4 32.2\\\\n300 23.2 29.2 35.3 38.6 43.7 45.9\\\\n600 24.0 30.1 36.5 40.8 46.6 50.4\\\\nTable 3: Comparison of architectures using models trained on the same data, with 640-dimensional\\\\nword vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,\\\\nand on the syntactic relationship test set of [20]\\\\nModel Semantic-Syntactic Word Relationship test set MSR Word Relatedness\\\\nArchitecture Semantic Accuracy [%] Syntactic Accuracy [%] Test Set [20]\\\\nRNNLM 9 36 35\\\\nNNLM 23 53 47\\\\nCBOW 24 64 61\\\\nSkip-gram 55 59 56\\\\n(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the\\\\nsame increase of computational complexity as increasing vector size twice.\\\\nFor the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-\\\\nent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so\\\\nthat it approaches zero at the end of the last training epoch.\\\\n4.3 Comparison of Model Architectures\\\\nFirst we compare different model architectures for deriving the word vectors using the same training\\\\ndata and using the same dimensionality of 640 of the word vectors. In the further experiments, we\\\\nuse full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to\\\\nthe 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic\\\\nsimilarity between words3.\\\\nThe training data consists of several LDC corpora and is described in detail in [18] (320M words,\\\\n82K vocabulary). We used these data to provide a comparison to a previously trained recurrent\\\\nneural network language model that took about 8 weeks to train on a single CPU. We trained a feed-\\\\nforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],\\\\nusing a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the\\\\nprojection layer has size 640 \\\\u00d7 8).\\\\nIn Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly\\\\non the syntactic questions. The NNLM vectors perform signi\\\\ufb01cantly better than the RNN - this is\\\\nnot surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden\\\\nlayer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the\\\\nsame on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic\\\\ntask than the CBOW model (but still better than the NNLM), and much better on the semantic part\\\\nof the test than all the other models.\\\\nNext, we evaluated our models trained using one CPU only and compared the results against publicly\\\\navailable word vectors. The comparison is given in Table 4. The CBOW model was trained on subset\\\\n3We thank Geoff Zweig for providing us the test set.\\\\n7\\\\n\\\\nTable 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-\\\\nship test set, and word vectors from our models. Full vocabularies are used.\\\\nModel Vector Training Accuracy [%]\\\\nDimensionality words\\\\nSemantic Syntactic Total\\\\nCollobert-Weston NNLM 50 660M 9.3 12.3 11.0\\\\nTurian NNLM 50 37M 1.4 2.6 2.1\\\\nTurian NNLM 200 37M 1.4 2.2 1.8\\\\nMnih NNLM 50 37M 1.8 9.1 5.8\\\\nMnih NNLM 100 37M 3.3 13.2 8.8\\\\nMikolov RNNLM 80 320M 4.9 18.4 12.7\\\\nMikolov RNNLM 640 320M 8.6 36.5 24.6\\\\nHuang NNLM 50 990M 13.3 11.6 12.3\\\\nOur NNLM 20 6B 12.9 26.4 20.3\\\\nOur NNLM 50 6B 27.9 55.8 43.2\\\\nOur NNLM 100 6B 34.2 64.5 50.8\\\\nCBOW 300 783M 15.5 53.1 36.1\\\\nSkip-gram 300 783M 50.0 55.9 53.3\\\\nTable 5: Comparison of models trained for three epochs on the same data and models trained for\\\\none epoch. Accuracy is reported on the full Semantic-Syntactic data set.\\\\nModel Vector Training Accuracy [%] Training time\\\\nDimensionality words [days]\\\\nSemantic Syntactic Total\\\\n3 epoch CBOW 300 783M 15.5 53.1 36.1 1\\\\n3 epoch Skip-gram 300 783M 50.0 55.9 53.3 3\\\\n1 epoch CBOW 300 783M 13.8 49.9 33.6 0.3\\\\n1 epoch CBOW 300 1.6B 16.1 52.6 36.1 0.6\\\\n1 epoch CBOW 600 783M 15.4 53.3 36.2 0.7\\\\n1 epoch Skip-gram 300 783M 45.6 52.2 49.2 1\\\\n1 epoch Skip-gram 300 1.6B 52.2 55.1 53.8 2\\\\n1 epoch Skip-gram 600 783M 56.7 54.5 55.5 2.5\\\\nof the Google News data in about a day, while training time for the Skip-gram model was about three\\\\ndays.\\\\nFor experiments reported further, we used just one training epoch (again, we decrease the learning\\\\nrate linearly so that it approaches zero at the end of training). Training a model on twice as much\\\\ndata using one epoch gives comparable or better results than iterating over the same data for three\\\\nepochs, as is shown in Table 5, and provides additional small speedup.\\\\n4.4 Large Scale Parallel Training of Models\\\\nAs mentioned earlier, we have implemented various models in a distributed framework called Dis-\\\\ntBelief. Below we report the results of several models trained on the Google News 6B data set,\\\\nwith mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-\\\\ngrad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an\\\\n8\\\\n\\\\nTable 6: Comparison of models trained using the DistBelief distributed framework. Note that\\\\ntraining of NNLM with 1000-dimensional vectors would take too long to complete.\\\\nModel Vector Training Accuracy [%] Training time\\\\nDimensionality words [days x CPU cores]\\\\nSemantic Syntactic Total\\\\nNNLM 100 6B 34.2 64.5 50.8 14 x 180\\\\nCBOW 1000 6B 57.3 68.9 63.7 2 x 140\\\\nSkip-gram 1000 6B 66.1 65.1 65.6 2.5 x 125\\\\nTable 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.\\\\nArchitecture Accuracy [%]\\\\n4-gram [32] 39\\\\nAverage LSA similarity [32] 49\\\\nLog-bilinear model [24] 54.8\\\\nRNNLMs [19] 55.4\\\\nSkip-gram 48.0\\\\nSkip-gram + RNNLMs 58.9\\\\nestimate since the data center machines are shared with other production tasks, and the usage can\\\\n\\\\ufb02uctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of\\\\nthe CBOW model and the Skip-gram model are much closer to each other than their single-machine\\\\nimplementations. The result are reported in Table 6.\\\\n4.5 Microsoft Research Sentence Completion Challenge\\\\nThe Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing\\\\nlanguage modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one\\\\nword is missing in each sentence and the goal is to select word that is the most coherent with the\\\\nrest of the sentence, given a list of \\\\ufb01ve reasonable choices. Performance of several techniques has\\\\nbeen already reported on this set, including N-gram models, LSA-based model [32], log-bilinear\\\\nmodel [24] and a combination of recurrent neural networks that currently holds the state of the art\\\\nperformance of 55.4% accuracy on this benchmark [19].\\\\nWe have explored the performance of Skip-gram architecture on this task. First, we train the 640-\\\\ndimensional model on 50M words provided in [32]. Then, we compute score of each sentence in\\\\nthe test set by using the unknown word at the input, and predict all surrounding words in a sentence.\\\\nThe \\\\ufb01nal sentence score is then the sum of these individual predictions. Using the sentence scores,\\\\nwe choose the most likely sentence.\\\\nA short summary of some previous results together with the new results is presented in Table 7.\\\\nWhile the Skip-gram model itself does not perform on this task better than LSA similarity, the scores\\\\nfrom this model are complementary to scores obtained with RNNLMs, and a weighted combination\\\\nleads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and\\\\n58.7% on the test part of the set).\\\\n5 Examples of the Learned Relationships\\\\nTable 8 shows words that follow various relationships. We follow the approach described above: the\\\\nrelationship is de\\\\ufb01ned by subtracting two word vectors, and the result is added to another word. Thus\\\\nfor example, Paris - France + Italy = Rome . As it can be seen, accuracy is quite good, although\\\\nthere is clearly a lot of room for further improvements (note that using our accuracy metric that\\\\n9\\\\n\\\\nTable 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-\\\\ngram model trained on 783M words with 300 dimensionality).\\\\nRelationship Example 1 Example 2 Example 3\\\\nFrance - Paris Italy: Rome Japan: Tokyo Florida: Tallahassee\\\\nbig - bigger small: larger cold: colder quick: quicker\\\\nMiami - Florida Baltimore: Maryland Dallas: Texas Kona: Hawaii\\\\nEinstein - scientist Messi: mid\\\\ufb01elder Mozart: violinist Picasso: painter\\\\nSarkozy - France Berlusconi: Italy Merkel: Germany Koizumi: Japan\\\\ncopper - Cu zinc: Zn gold: Au uranium: plutonium\\\\nBerlusconi - Silvio Sarkozy: Nicolas Putin: Medvedev Obama: Barack\\\\nMicrosoft - Windows Google: Android IBM: Linux Apple: iPhone\\\\nMicrosoft - Ballmer Google: Yahoo IBM: McNealy Apple: Jobs\\\\nJapan - sushi Germany: bratwurst France: tapas USA: pizza\\\\nassumes exact match, the results in Table 8 would score only about 60%). We believe that word\\\\nvectors trained on even larger data sets with larger dimensionality will perform signi\\\\ufb01cantly better,\\\\nand will enable the development of new innovative applications. Another way to improve accuracy is\\\\nto provide more than one example of the relationship. By using ten examples instead of one to form\\\\nthe relationship vector (we average the individual vectors together), we have observed improvement\\\\nof accuracy of our best models by about 10% absolutely on the semantic-syntactic test.\\\\nIt is also possible to apply the vector operations to solve different tasks. For example, we have\\\\nobserved good accuracy for selecting out-of-the-list words, by computing average vector for a list of\\\\nwords, and \\\\ufb01nding the most distant word vector. This is a popular type of problems in certain human\\\\nintelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.\\\\n6 Conclusion\\\\nIn this paper we studied the quality of vector representations of words derived by various models on\\\\na collection of syntactic and semantic language tasks. We observed that it is possible to train high\\\\nquality word vectors using very simple model architectures, compared to the popular neural network\\\\nmodels (both feedforward and recurrent). Because of the much lower computational complexity, it\\\\nis possible to compute very accurate high dimensional word vectors from a much larger data set.\\\\nUsing the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram\\\\nmodels even on corpora with one trillion words, for basically unlimited size of the vocabulary. That\\\\nis several orders of magnitude larger than the best previously published results for similar models.\\\\nAn interesting task where the word vectors have recently been shown to signi\\\\ufb01cantly outperform the\\\\nprevious state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were\\\\nused together with other techniques to achieve over 50% increase in Spearman\\\\u2019s rank correlation\\\\nover the previous best result [31]. The neural network based word vectors were previously applied\\\\nto many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can\\\\nbe expected that these applications can bene\\\\ufb01t from the model architectures described in this paper.\\\\nOur ongoing work shows that the word vectors can be successfully applied to automatic extension\\\\nof facts in Knowledge Bases, and also for veri\\\\ufb01cation of correctness of existing facts. Results\\\\nfrom machine translation experiments also look very promising. In the future, it would be also\\\\ninteresting to compare our techniques to Latent Relational Analysis [30] and others. We believe that\\\\nour comprehensive test set will help the research community to improve the existing techniques for\\\\nestimating the word vectors. We also expect that high quality word vectors will become an important\\\\nbuilding block for future NLP applications.\\\\n10\\\\n\\\\n7 Follow-Up Work\\\\nAfter the initial version of this paper was written, we published single-machine multi-threaded C++\\\\ncode for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-\\\\ntectures4. The training speed is signi\\\\ufb01cantly higher than reported earlier in this paper, i.e. it is in the\\\\norder of billions of words per hour for typical hyperparameter choices. We also published more than\\\\n1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of\\\\nour follow-up work will be published in an upcoming NIPS 2013 paper [21].\\\\nReferences\\\\n[1] Y . Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-\\\\nchine Learning Research, 3:1137-1155, 2003.\\\\n[2] Y . Bengio, Y . LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-\\\\nchines, MIT Press, 2007.\\\\n[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine\\\\ntranslation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language\\\\nProcessing and Computational Language Learning, 2007.\\\\n[4] R. Collobert and J. Weston. A Uni\\\\ufb01ed Architecture for Natural Language Processing: Deep\\\\nNeural Networks with Multitask Learning. In International Conference on Machine Learning,\\\\nICML, 2008.\\\\n[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-\\\\nguage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-\\\\n2537, 2011.\\\\n[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V . Le, M.Z. Mao, M.A. Ranzato, A.\\\\nSenior, P. Tucker, K. Yang, A. Y . Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\\\\n[7] J.C. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and\\\\nstochastic optimization. Journal of Machine Learning Research, 2011.\\\\n[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.\\\\n[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y . Ng. Improving Word Representations\\\\nvia Global Context and Multiple Word Prototypes. In: Proc. Association for Computational\\\\nLinguistics, 2012.\\\\n[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-\\\\ntributed processing: Explorations in the microstructure of cognition. V olume 1: Foundations,\\\\nMIT Press, 1986.\\\\n[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring\\\\ndegrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic\\\\nEvaluation (SemEval 2012), 2012.\\\\n[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y . Ng, and C. Potts. Learning word vectors for\\\\nsentiment analysis. In Proceedings of ACL, 2011.\\\\n[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-\\\\nversity of Technology, 2007.\\\\n[14] T. Mikolov, J. Kopeck \\\\u00b4y, L. Burget, O. Glembek and J. \\\\u02c7Cernock\\\\u00b4y. Neural network based lan-\\\\nguage models for higly in\\\\ufb02ective languages, In: Proc. ICASSP 2009.\\\\n[15] T. Mikolov, M. Kara\\\\ufb01 \\\\u00b4at, L. Burget, J. \\\\u02c7Cernock\\\\u00b4y, S. Khudanpur. Recurrent neural network\\\\nbased language model, In: Proceedings of Interspeech, 2010.\\\\n[16] T. Mikolov, S. Kombrink, L. Burget, J. \\\\u02c7Cernock\\\\u00b4y, S. Khudanpur. Extensions of recurrent neural\\\\nnetwork language model, In: Proceedings of ICASSP 2011.\\\\n[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. \\\\u02c7Cernock\\\\u00b4y. Empirical Evaluation and Com-\\\\nbination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\\\\n4The code is available at https://code.google.com/p/word2vec/\\\\n11\\\\n\\\\n[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. \\\\u02c7Cernock\\\\u00b4y. Strategies for Training Large Scale\\\\nNeural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-\\\\ning, 2011.\\\\n[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-\\\\nsity of Technology, 2012.\\\\n[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-\\\\ntations. NAACL HLT 2013.\\\\n[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of\\\\nWords and Phrases and their Compositionality. Accepted to NIPS 2013.\\\\n[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,\\\\n2007.\\\\n[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural\\\\nInformation Processing Systems 21, MIT Press, 2009.\\\\n[24] A. Mnih, Y .W. Teh. A fast and simple algorithm for training neural probabilistic language\\\\nmodels. ICML, 2012.\\\\n[25] F. Morin, Y . Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,\\\\n2005.\\\\n[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-\\\\npropagating errors. Nature, 323:533.536, 1986.\\\\n[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,\\\\n2007.\\\\n[28] R. Socher, E.H. Huang, J. Pennington, A.Y . Ng, and C.D. Manning. Dynamic Pooling and\\\\nUnfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.\\\\n[29] J. Turian, L. Ratinov, Y . Bengio. Word Representations: A Simple and General Method for\\\\nSemi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.\\\\n[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-\\\\ntional Joint Conference on Arti\\\\ufb01cial Intelligence, 2005.\\\\n[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for\\\\nMeasuring Relational Similarity. NAACL HLT 2013.\\\\n[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft\\\\nResearch Technical Report MSR-TR-2011-129, 2011.\\\\n12\"}'}]\n",
      "Final output:\n",
      "{\n",
      "  \"id\": \"resp_006bc7ff483635a100695cf77da0f4819782aca9acf9f9d4d2\",\n",
      "  \"created_at\": 1767700349.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": \"Summarize the PDF content provided via the tool.\",\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-5-2025-08-07\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"rs_006bc7ff483635a100695cf77e63e88197bdaca3c8c705431e\",\n",
      "      \"summary\": [],\n",
      "      \"type\": \"reasoning\",\n",
      "      \"content\": null,\n",
      "      \"encrypted_content\": null,\n",
      "      \"status\": null\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_006bc7ff483635a100695cf783b0e881979cd70b426f10c890\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"Here’s a concise summary of the paper “Efficient Estimation of Word Representations in Vector Space” (Mikolov et al., 2013):\\n\\nProblem and goal\\n- Learn high-quality, dense word vectors (embeddings) from very large corpora efficiently, preserving both syntactic and semantic regularities.\\n- Enable scalability to billions of tokens and million-word vocabularies, with practical training times.\\n\\nKey contributions\\n- Two simple, efficient log-linear architectures:\\n  - CBOW (Continuous Bag-of-Words): predicts the current word from the average of surrounding context words.\\n  - Skip-gram: predicts surrounding context words given the current word.\\n- Use of hierarchical softmax with a Huffman tree to cut output complexity, enabling fast training.\\n- Parallel, large-scale training on Google’s DistBelief to scale to billions of tokens.\\n\\nComplexity insights\\n- Removing nonlinear hidden layers (vs NNLM/RNNLM) dramatically reduces training cost.\\n- Skip-gram trades computation for better semantic quality by predicting multiple context words per center word.\\n- Demonstrates diminishing returns: need to grow both dimensionality and data together for best gains.\\n\\nEvaluation and results\\n- Introduces a comprehensive analogy-style benchmark with 5 semantic and 9 syntactic relation types (e.g., “king - man + woman ≈ queen; Paris - France + Italy ≈ Rome”).\\n- Architecture comparison (same data, 640 dims):\\n  - RNNLM: strong on syntax, weaker on semantics.\\n  - NNLM: better than RNNLM overall.\\n  - CBOW: best on syntax among these.\\n  - Skip-gram: substantially best on semantics; competitive on syntax.\\n- Large-scale comparisons:\\n  - On Google News (6B tokens), 1000-d:\\n    - CBOW: 63.7% total (semantic 57.3, syntactic 68.9).\\n    - Skip-gram: 65.6% total (semantic 66.1, syntactic 65.1).\\n- Training efficiency:\\n  - High-quality vectors learned in under a day on ~1.6B tokens (single machine for smaller models; distributed for larger).\\n  - One-pass training on more data can match or beat multi-epoch training on less data.\\n- MSR Sentence Completion:\\n  - Skip-gram alone: 48% (below top systems), but complementary to RNNLMs; combined system achieves new SOTA at 58.9%.\\n\\nTakeaways\\n- Simple CBOW/Skip-gram models (word2vec) learn embeddings that capture linear semantic/syntactic relationships and support vector arithmetic for analogies.\\n- Skip-gram excels at semantic relations; CBOW is strong on syntactic ones.\\n- With hierarchical softmax and distributed training, these models scale efficiently to very large corpora, outperforming prior embeddings and rivaling more complex neural LMs in downstream utility.\\n- Vectors are broadly useful across NLP tasks (e.g., MT, IR, QA, sentiment) and can aid knowledge base completion.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"name\": \"load_pdf_text\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"file_path\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"Path to the PDF file to extract text from.\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"file_path\"\n",
      "        ],\n",
      "        \"additionalProperties\": false\n",
      "      },\n",
      "      \"strict\": true,\n",
      "      \"type\": \"function\",\n",
      "      \"description\": \"Load and return the full text from the provided PDF file path.\"\n",
      "    }\n",
      "  ],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"conversation\": null,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"prompt\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"prompt_cache_retention\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": \"medium\",\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"safety_identifier\": null,\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": \"medium\"\n",
      "  },\n",
      "  \"top_logprobs\": 0,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 10158,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 948,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 320\n",
      "    },\n",
      "    \"total_tokens\": 11106\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"billing\": {\n",
      "    \"payer\": \"developer\"\n",
      "  },\n",
      "  \"completed_at\": 1767700367,\n",
      "  \"store\": true\n",
      "}\n",
      "\n",
      "Here’s a concise summary of the paper “Efficient Estimation of Word Representations in Vector Space” (Mikolov et al., 2013):\n",
      "\n",
      "Problem and goal\n",
      "- Learn high-quality, dense word vectors (embeddings) from very large corpora efficiently, preserving both syntactic and semantic regularities.\n",
      "- Enable scalability to billions of tokens and million-word vocabularies, with practical training times.\n",
      "\n",
      "Key contributions\n",
      "- Two simple, efficient log-linear architectures:\n",
      "  - CBOW (Continuous Bag-of-Words): predicts the current word from the average of surrounding context words.\n",
      "  - Skip-gram: predicts surrounding context words given the current word.\n",
      "- Use of hierarchical softmax with a Huffman tree to cut output complexity, enabling fast training.\n",
      "- Parallel, large-scale training on Google’s DistBelief to scale to billions of tokens.\n",
      "\n",
      "Complexity insights\n",
      "- Removing nonlinear hidden layers (vs NNLM/RNNLM) dramatically reduces training cost.\n",
      "- Skip-gram trades computation for better semantic quality by predicting multiple context words per center word.\n",
      "- Demonstrates diminishing returns: need to grow both dimensionality and data together for best gains.\n",
      "\n",
      "Evaluation and results\n",
      "- Introduces a comprehensive analogy-style benchmark with 5 semantic and 9 syntactic relation types (e.g., “king - man + woman ≈ queen; Paris - France + Italy ≈ Rome”).\n",
      "- Architecture comparison (same data, 640 dims):\n",
      "  - RNNLM: strong on syntax, weaker on semantics.\n",
      "  - NNLM: better than RNNLM overall.\n",
      "  - CBOW: best on syntax among these.\n",
      "  - Skip-gram: substantially best on semantics; competitive on syntax.\n",
      "- Large-scale comparisons:\n",
      "  - On Google News (6B tokens), 1000-d:\n",
      "    - CBOW: 63.7% total (semantic 57.3, syntactic 68.9).\n",
      "    - Skip-gram: 65.6% total (semantic 66.1, syntactic 65.1).\n",
      "- Training efficiency:\n",
      "  - High-quality vectors learned in under a day on ~1.6B tokens (single machine for smaller models; distributed for larger).\n",
      "  - One-pass training on more data can match or beat multi-epoch training on less data.\n",
      "- MSR Sentence Completion:\n",
      "  - Skip-gram alone: 48% (below top systems), but complementary to RNNLMs; combined system achieves new SOTA at 58.9%.\n",
      "\n",
      "Takeaways\n",
      "- Simple CBOW/Skip-gram models (word2vec) learn embeddings that capture linear semantic/syntactic relationships and support vector arithmetic for analogies.\n",
      "- Skip-gram excels at semantic relations; CBOW is strong on syntactic ones.\n",
      "- With hierarchical softmax and distributed training, these models scale efficiently to very large corpora, outperforming prior embeddings and rivaling more complex neural LMs in downstream utility.\n",
      "- Vectors are broadly useful across NLP tasks (e.g., MT, IR, QA, sentiment) and can aid knowledge base completion.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# 1. Define a list of callable tools for the model\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"load_pdf_text\",\n",
    "        \"description\": \"Load and return the full text from the provided PDF file path.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"file_path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Path to the PDF file to extract text from.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"file_path\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    \"\"\"Loads text from a PDF file.\"\"\"\n",
    "    from pypdf import PdfReader\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    return text\n",
    "\n",
    "# Create a running input list we will add to over time\n",
    "input_list = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you read and summarize the file at ../assets/paper3.pdf?\"}\n",
    "]\n",
    "\n",
    "# 2. Prompt the model with tools defined\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    tools=tools,\n",
    "    input=input_list,\n",
    ")\n",
    "\n",
    "# Save function call outputs for subsequent requests\n",
    "input_list += response.output\n",
    "\n",
    "for item in response.output:\n",
    "    if item.type == \"function_call\":\n",
    "        if item.name == \"load_pdf_text\":\n",
    "            # 3. Execute the function logic for load_pdf_text\n",
    "            file_args = json.loads(item.arguments)\n",
    "            pdf_text = load_pdf_text(file_args[\"file_path\"])\n",
    "\n",
    "            # 4. Provide function call results to the model\n",
    "            input_list.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": item.call_id,\n",
    "                \"output\": json.dumps({\n",
    "                  \"pdf_text\": pdf_text\n",
    "                })\n",
    "            })\n",
    "\n",
    "print(\"Final input:\")\n",
    "print(input_list)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    instructions=\"Summarize the PDF content provided via the tool.\",\n",
    "    tools=tools,\n",
    "    input=input_list,\n",
    ")\n",
    "\n",
    "# 5. The model should be able to give a response!\n",
    "print(\"Final output:\")\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(\"\\n\" + response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06db926",
   "metadata": {},
   "source": [
    "<img src=\"../assets/functions_schema.png\" style=\"width: 50%;\" />\n",
    "\n",
    "https://platform.openai.com/docs/guides/function-calling#defining-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626cd85",
   "metadata": {},
   "source": [
    "# Managing Conversation States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62db3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3 core skills for snowboarding are:\n",
      "\n",
      "1. **Balance and Stance** – Maintaining proper body positioning and weight distribution on the board to stay stable and in control.\n",
      "\n",
      "2. **Turning** – Learning how to carve and control direction primarily through edge control and shifting weight to execute smooth turns.\n",
      "\n",
      "3. **Speed Control and Stopping** – Managing your speed safely using techniques like skidding, carving, and performing controlled stops.\n",
      "\n",
      "Mastering these foundational skills helps build confidence and sets the stage for more advanced tricks and maneuvers.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from openai.types.shared import reasoning_effort\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"What are the 3 core skills for snowboarding?\"},\n",
    "    ],\n",
    "    \n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f20fe75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What are the 3 core skills for snowboarding?'},\n",
       " {'role': 'assistant',\n",
       "  'content': [ResponseOutputText(annotations=[], text='The 3 core skills for snowboarding are:\\n\\n1. **Balance and Stance** – Maintaining proper body positioning and weight distribution on the board to stay stable and in control.\\n\\n2. **Turning** – Learning how to carve and control direction primarily through edge control and shifting weight to execute smooth turns.\\n\\n3. **Speed Control and Stopping** – Managing your speed safely using techniques like skidding, carving, and performing controlled stops.\\n\\nMastering these foundational skills helps build confidence and sets the stage for more advanced tricks and maneuvers.', type='output_text', logprobs=[])]}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the response to the conversation\n",
    "history = [\n",
    "        {\"role\": \"user\", \"content\": \"What are the 3 core skills for snowboarding?\"},\n",
    "    ]\n",
    "history += [{\"role\": el.role, \"content\": el.content} for el in response.output]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0b1c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here are 3 exercises for each core snowboarding skill:\n",
      "\n",
      "**1. Balance and Stance**  \n",
      "- Practice balancing on one foot on flat ground (or on a balance board if available).  \n",
      "- Side-to-side weight shifts while strapped into your board on gentle terrain.  \n",
      "- Static holds: Standing still in your proper snowboarding stance with knees slightly bent and eyes forward.\n",
      "\n",
      "**2. Turning**  \n",
      "- Toe-edge and heel-edge slips down a gentle slope to feel the edges biting into the snow.  \n",
      "- Linking wide S-shaped turns on a beginner slope, focusing on smooth weight transitions.  \n",
      "- Traverse across the slope and gradually switch edges to initiate turning.\n",
      "\n",
      "**3. Speed Control and Stopping**  \n",
      "- Controlled falling leaf exercise: slide down while shifting weight to control speed without fully turning downhill.  \n",
      "- Practice hockey stops by carving onto both edges and digging in to come to a quick stop.  \n",
      "- Gradually increase speed on gentle terrain and practice controlled slow-down skidding turns.\n",
      "\n",
      "These exercises help build muscle memory and confidence in each fundamental area.\n"
     ]
    }
   ],
   "source": [
    "second_input = {\"role\": \"user\", \"content\": \"For each of these give me 3 exercises to practice in simple bullets.\"}\n",
    "\n",
    "history.append(second_input)\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=history,\n",
    ")\n",
    "\n",
    "print(second_response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0391fde5",
   "metadata": {},
   "source": [
    "# Conversations API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17a3924",
   "metadata": {},
   "source": [
    "To facilitate conversation management and avoid having to pass inputs manually like this, OpenAI created a special API to work with the Responses API called: \"Conversations API\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db08fc0",
   "metadata": {},
   "source": [
    "The [Conversations API](https://platform.openai.com/docs/api-reference/conversations/create) works with the Responses API to persist conversation state as a long-running object with its own durable identifier. After creating a conversation object, you can keep using it across sessions, devices, or jobs.\n",
    "\n",
    "Conversations store items, which can be messages, tool calls, tool outputs, and other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adcc9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a programmer, several skills are crucial for success in the field. Here are three of the most important:\n",
      "\n",
      "1. **Problem-Solving Skills**:\n",
      "   - The ability to analyze complex problems, break them down into manageable parts, and devise effective solutions is essential. This involves logical thinking and creativity to address challenges and optimize code.\n",
      "\n",
      "2. **Proficiency in Algorithms and Data Structures**:\n",
      "   - Understanding algorithms and data structures is fundamental. This knowledge helps in writing efficient code, improving performance, and making informed decisions when choosing the right approach for a given problem.\n",
      "\n",
      "3. **Collaboration and Communication**:\n",
      "   - Programming is often a team effort. Strong communication skills facilitate better collaboration with team members, understanding requirements, and conveying ideas effectively. This includes documentation skills and the ability to provide constructive feedback. \n",
      "\n",
      "Focusing on these areas will greatly enhance your effectiveness and adaptability as a programmer.\n",
      "Sure! Here are two exercises for each of the three important skills:\n",
      "\n",
      "### 1. Problem-Solving Skills\n",
      "**Exercise 1: Daily Coding Challenge**\n",
      "- Choose a problem from platforms like LeetCode, HackerRank, or Codewars every day. Start with easy problems and gradually progress to more challenging ones. Focus on understanding the problem, outlining your solution, and then implementing it.\n",
      "\n",
      "**Exercise 2: Real-World Scenario**\n",
      "- Identify a repetitive task you do manually (like organizing files, data entry, etc.). Design a program or a script to automate it. This will help you think critically about breaking down a real-world problem into smaller, solvable parts.\n",
      "\n",
      "### 2. Proficiency in Algorithms and Data Structures\n",
      "**Exercise 1: Implement Common Algorithms**\n",
      "- Pick a few fundamental algorithms (like sorting algorithms: bubble sort, quicksort) and implement them from scratch in your preferred programming language. Analyze their time and space complexities and understand the trade-offs.\n",
      "\n",
      "**Exercise 2: Data Structure Implementation**\n",
      "- Choose a data structure (like a linked list, stack, or queue) and implement it from scratch. Then, perform basic operations (insertion, deletion, traversal) and analyze their efficiencies.\n",
      "\n",
      "### 3. Collaboration and Communication\n",
      "**Exercise 1: Code Review Practice**\n",
      "- Participate in a code review process. Exchange code with a peer and provide feedback on their code while also being open to receiving feedback on your own. Focus on clarity, readability, and adherence to best practices.\n",
      "\n",
      "**Exercise 2: Document Your Project**\n",
      "- Take a small project you've worked on and create comprehensive documentation. Include an overview, installation steps, usage examples, and any design decisions made during development. This will help you practice elucidating technical concepts for different audiences. \n",
      "\n",
      "Engaging in these exercises will greatly enhance your skills across these critical areas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"What are the 3 most important skills to develop as a programmer.\",\n",
    ")\n",
    "print(response.output_text)\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    previous_response_id=response.id,\n",
    "    input=[{\"role\": \"user\", \"content\": \"Write down 2 exercises for each of those skills.\"}],\n",
    ")\n",
    "print(second_response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# iniitliaze a conversation session with an ID\n",
    "def create_conversation(user_id: str) -> str:\n",
    "    # 1. Create a new conversation container\n",
    "    conversation = client.conversations.create(\n",
    "        metadata={\"user_id\": user_id, \"status\": \"active\"}\n",
    "    )\n",
    "    return conversation.id\n",
    "\n",
    "# something to send messages\n",
    "def send_message(conv_id: str, user_text: str) -> str:\n",
    "    # 2. Send a turn through Responses, bound to that conversation\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=[{\"role\": \"user\", \"content\": user_text}],\n",
    "        conversation=conv_id,\n",
    "        # store=True  # optional; default is stored for conversation state\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "# ability to update conversation metadata\n",
    "def update_tag_conversation(conv_id: str, topic: str) -> None:\n",
    "    # 3. Update metadata on the conversation (for filtering, dashboards, etc.)\n",
    "    client.conversations.update(\n",
    "        conversation_id=conv_id,\n",
    "        metadata={\"topic\": topic}\n",
    "    )\n",
    "\n",
    "user_id = \"user-123\"\n",
    "conv_id = create_conversation(user_id)\n",
    "\n",
    "msg = \"Hi, I'm Lucas and I want help deciding on my new laptop. \\\n",
    "    Give me just a sentence with a simple decision making framework.\"\n",
    "answer = send_message(conv_id, msg)\n",
    "print(\"Assistant:\", answer)\n",
    "\n",
    "# maybe after inspecting the first message:\n",
    "update_tag_conversation(conv_id, topic=\"shopping-help\")\n",
    "msg = \"What's my name and what am I buying?\"\n",
    "followup = send_message(conv_id, msg)\n",
    "print(\"Assistant:\", followup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cea89",
   "metadata": {},
   "source": [
    "# Managing Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355b3fb",
   "metadata": {},
   "source": [
    "![Context window illustration](https://cdn.openai.com/API/docs/images/context-window.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6097c42",
   "metadata": {},
   "source": [
    "- Context is managed in tokens\n",
    "- Tokens are just simple units of text\n",
    "- [Context window limits for each OpenAI Model](https://platform.openai.com/docs/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def get_num_tokens(prompt, model=\"gpt-5\"):\n",
    "    \"\"\"Calculates the number of tokens in a text prompt\"\"\"\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    return len(enc.encode(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd9520f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens(\"Hello folks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b198f00",
   "metadata": {},
   "source": [
    "## Token Cost & Context Windows for GPT-5-... models\n",
    "\n",
    "__GPT-5.2__\n",
    "- https://platform.openai.com/docs/models/gpt-5.2\n",
    "\n",
    "__GPT-5.1__\n",
    "- https://platform.openai.com/docs/models/gpt-5.1\n",
    "\n",
    "__GPT-5__\n",
    "- https://platform.openai.com/docs/models/gpt-5 \n",
    "\n",
    "__GPT-5-mini__\n",
    "- https://platform.openai.com/docs/models/gpt-5-mini"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
