{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2740bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from pypdf import PdfReader\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class PaperData(BaseModel):\n",
    "    title: str = Field(description=\"The title of the paper\")\n",
    "    abstract_summary: List[str] = Field(description=\"One sentence summary of the abstract\")\n",
    "    intro_backgrounds_summary: str = Field(description=\"One sentence summary of the intro and backgrounds sections\")\n",
    "    methods_summary: str = Field(description=\"One sentence summary of the methods section\")\n",
    "    results_summary: str = Field(description=\"One sentence summary of the results section\")\n",
    "    discussion_summary: str = Field(description=\"One sentence summary of the discussion section\")\n",
    "    \n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    '''Loads text from a PDF file.'''\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(file_path)\n",
    "\n",
    "    # extracting text from page\n",
    "    text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    \n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1695146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-26 13:45:11--  https://arxiv.org/pdf/2510.26493\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.3.42, 151.101.67.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2371045 (2,3M) [application/pdf]\n",
      "Saving to: ‘../assets/paper.pdf’\n",
      "\n",
      "../assets/paper.pdf 100%[===================>]   2,26M  --.-KB/s    in 0,07s   \n",
      "\n",
      "2025-11-26 13:45:11 (32,0 MB/s) - ‘../assets/paper.pdf’ saved [2371045/2371045]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ../assets/paper.pdf https://arxiv.org/pdf/2510.26493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea3748",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_raw_text = load_pdf_text(\"../assets/paper.pdf\")\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=f\"extract the information from the paper: {paper_raw_text}\",\n",
    "    text_format=PaperData\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b214a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaperData(title='Context Engineering 2.0: The Context of Context Engineering', abstract_summary=['The paper argues that context engineering—designing, organizing, and managing contextual information for machines—is a long‑evolving discipline (not just a recent LLM-era invention), frames context engineering as an entropy‑reduction problem, presents a formal definition and four-stage evolutionary model (Context Engineering 1.0–4.0), and surveys practical design considerations for context collection, storage, management, and usage to guide future AI systems.'], intro_backgrounds_summary='Introduces context engineering, situates it historically from ubiquitous computing and early HCI to modern LLMs and agents, defines context broadly (information characterizing relevant entities), and frames the core challenge as bridging human intent and machine understanding by compressing high-entropy human contexts into machine‑usable representations.', methods_summary='Presents a formal mathematical framework (entity characterization Char, context C as aggregation over relevant entities), defines context engineering as f_context(C,T)=F(ϕ1,…,ϕn) (a composition of modular operations for collection, storage, representation, multimodal handling, selection, sharing and adaptation), and characterizes four developmental stages aligned with machine intelligence (Primitive/1.0, Agent‑centric/2.0, Human‑level/3.0, Superhuman/4.0).', results_summary='Through historical analysis and system-level comparison, the paper contrasts Era 1.0 and 2.0 practices (sensor/collection modes, tolerance for raw context, core mechanisms like Context Toolkit vs. prompting/RAG/memory agents), surveys contemporary designs (hierarchical memory, subagents, embeddings, schema extraction, KV caching, multimodal fusion), and synthesizes common patterns and trade-offs for context collection, management, and usage across applications.', discussion_summary='Identifies key challenges and future directions—scalable lifelong context storage and semantic indexing, long-context processing and architectural limits of transformers, robustness and evaluation of accumulated memory, cross‑system context sharing standards, richer multimodal (and BCI) sensing, and the need for a semantic operating system and new long-range reasoning architectures to enable reliable, proactive, and human-aligned context engineering. ')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e0d9ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Context Engineering 2.0: The Context of Context Engineering\n",
       "\n",
       "## Abstract Summary\n",
       "The paper argues that context engineering—designing, organizing, and managing contextual information for machines—is a long‑evolving discipline (not just a recent LLM-era invention), frames context engineering as an entropy‑reduction problem, presents a formal definition and four-stage evolutionary model (Context Engineering 1.0–4.0), and surveys practical design considerations for context collection, storage, management, and usage to guide future AI systems.\n",
       "\n",
       "## Introduction & Backgrounds Summary\n",
       "Introduces context engineering, situates it historically from ubiquitous computing and early HCI to modern LLMs and agents, defines context broadly (information characterizing relevant entities), and frames the core challenge as bridging human intent and machine understanding by compressing high-entropy human contexts into machine‑usable representations.\n",
       "\n",
       "## Methods Summary\n",
       "Presents a formal mathematical framework (entity characterization Char, context C as aggregation over relevant entities), defines context engineering as f_context(C,T)=F(ϕ1,…,ϕn) (a composition of modular operations for collection, storage, representation, multimodal handling, selection, sharing and adaptation), and characterizes four developmental stages aligned with machine intelligence (Primitive/1.0, Agent‑centric/2.0, Human‑level/3.0, Superhuman/4.0).\n",
       "\n",
       "## Results Summary\n",
       "Through historical analysis and system-level comparison, the paper contrasts Era 1.0 and 2.0 practices (sensor/collection modes, tolerance for raw context, core mechanisms like Context Toolkit vs. prompting/RAG/memory agents), surveys contemporary designs (hierarchical memory, subagents, embeddings, schema extraction, KV caching, multimodal fusion), and synthesizes common patterns and trade-offs for context collection, management, and usage across applications.\n",
       "\n",
       "## Discussion Summary\n",
       "Identifies key challenges and future directions—scalable lifelong context storage and semantic indexing, long-context processing and architectural limits of transformers, robustness and evaluation of accumulated memory, cross‑system context sharing standards, richer multimodal (and BCI) sensing, and the need for a semantic operating system and new long-range reasoning architectures to enable reliable, proactive, and human-aligned context engineering. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "md = f\"\"\"\n",
    "# {response.output_parsed.title}\n",
    "\n",
    "## Abstract Summary\n",
    "{'; '.join(response.output_parsed.abstract_summary)}\n",
    "\n",
    "## Introduction & Backgrounds Summary\n",
    "{response.output_parsed.intro_backgrounds_summary}\n",
    "\n",
    "## Methods Summary\n",
    "{response.output_parsed.methods_summary}\n",
    "\n",
    "## Results Summary\n",
    "{response.output_parsed.results_summary}\n",
    "\n",
    "## Discussion Summary\n",
    "{response.output_parsed.discussion_summary}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30883e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new entry to 'papers_database.csv' for title='Context Engineering 2.0: The Context of Context Engineering'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def upsert_paper_to_csv(paper_data, csv_path=\"papers_database.csv\", key_column=\"title\"):\n",
    "    \"\"\"\n",
    "    Inserts or updates a paper's data in a CSV file database. If the file does not exist,\n",
    "    it creates it. If the file exists, it updates the entry if the key exists, or appends \n",
    "    the new paper if not.\n",
    "\n",
    "    Parameters:\n",
    "        paper_data: an object with attributes corresponding to extracted fields\n",
    "        csv_path: output file path for CSV\n",
    "        key_column: column name to use as unique identifier (default: \"title\")\n",
    "    \"\"\"\n",
    "    # Prepare data dictionary, flattening abstract_summary if it's a list\n",
    "    paper_dict = {\n",
    "        \"title\": getattr(paper_data, \"title\", \"\"),\n",
    "        \"abstract_summary\": \"; \".join(paper_data.abstract_summary) if isinstance(getattr(paper_data, \"abstract_summary\", \"\"), list) else getattr(paper_data, \"abstract_summary\", \"\"),\n",
    "        \"intro_backgrounds_summary\": getattr(paper_data, \"intro_backgrounds_summary\", \"\"),\n",
    "        \"methods_summary\": getattr(paper_data, \"methods_summary\", \"\"),\n",
    "        \"results_summary\": getattr(paper_data, \"results_summary\", \"\"),\n",
    "        \"discussion_summary\": getattr(paper_data, \"discussion_summary\", \"\"),\n",
    "    }\n",
    "\n",
    "    # If CSV exists, load and update if key matches; otherwise create new\n",
    "    if os.path.isfile(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Check if key (e.g., title) already in db\n",
    "        if paper_dict[key_column] in df[key_column].values:\n",
    "            # Update existing entry\n",
    "            df.loc[df[key_column] == paper_dict[key_column], list(paper_dict.keys())] = list(paper_dict.values())\n",
    "            updated = True\n",
    "        else:\n",
    "            # Append if not found\n",
    "            df = pd.concat([df, pd.DataFrame([paper_dict])], ignore_index=True)\n",
    "            updated = False\n",
    "    else:\n",
    "        # Create new dataframe\n",
    "        df = pd.DataFrame([paper_dict])\n",
    "        updated = False\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    if updated:\n",
    "        print(f\"Updated entry in '{csv_path}' for {key_column}='{paper_dict[key_column]}'\")\n",
    "    else:\n",
    "        print(f\"Added new entry to '{csv_path}' for {key_column}='{paper_dict[key_column]}'\")\n",
    "\n",
    "# Example usage:\n",
    "upsert_paper_to_csv(response.output_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd483d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-25 16:25:46--  https://arxiv.org/pdf/1301.3781\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.195.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 228716 (223K) [application/pdf]\n",
      "Saving to: ‘paper3.pdf’\n",
      "\n",
      "paper3.pdf          100%[===================>] 223,36K  --.-KB/s    in 0,02s   \n",
      "\n",
      "2025-11-25 16:25:46 (9,24 MB/s) - ‘paper3.pdf’ saved [228716/228716]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ../assets/paper3.pdf \"https://arxiv.org/pdf/1301.3781\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cb718b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Efficient Estimation of Word Representations in Vector Space' abstract_summary=['The paper introduces two efficient log-linear architectures (Continuous Bag‑of‑Words and Skip‑gram) for learning continuous word vectors from very large corpora, achieving large accuracy improvements on syntactic and semantic word similarity tasks at much lower computational cost (e.g. learning high‑quality vectors from ~1.6B words in under a day).'] intro_backgrounds_summary='The paper motivates learning distributed continuous word representations (instead of atomic indices) to capture multiple degrees of word similarity and linear regularities (e.g., vector arithmetic like king−man+woman≈queen), surveys prior neural language model work, and sets the goal of scalable, high‑quality word vectors from very large datasets and vocabularies.' methods_summary='They propose two computationally cheap log‑linear models—CBOW (predict target word from averaged context vectors) and Skip‑gram (predict context words from target word)—use hierarchical softmax with Huffman trees and parallel training (DistBelief/Adagrad), and analyze training complexity and hyperparameters (vector dimensionality, context size, epochs).' results_summary='Using a new Semantic‑Syntactic evaluation (≈8.9k semantic, 10.7k syntactic questions) and large corpora (up to Google News 6B tokens), Skip‑gram yields strong semantic accuracy while CBOW is faster and good on syntactic tasks; both outperform prior embeddings, scale to high dimensions and data (e.g., 1000‑d vectors trained in days with distributed training), and combining embeddings with RNNLMs improves downstream tasks (MSRC sentence completion SOTA).' discussion_summary='Simple, scalable architectures (CBOW/Skip‑gram) produce high‑quality, linearly regular word vectors that can be trained on orders‑of‑magnitude larger corpora than prior work, are useful across NLP applications, have been released (word2vec) and can be further scaled and improved in future work.'\n",
      "Added new entry to 'papers_database.csv' for title='Efficient Estimation of Word Representations in Vector Space'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PaperData(title='Efficient Estimation of Word Representations in Vector Space', abstract_summary=['The paper introduces two efficient log-linear architectures (Continuous Bag‑of‑Words and Skip‑gram) for learning continuous word vectors from very large corpora, achieving large accuracy improvements on syntactic and semantic word similarity tasks at much lower computational cost (e.g. learning high‑quality vectors from ~1.6B words in under a day).'], intro_backgrounds_summary='The paper motivates learning distributed continuous word representations (instead of atomic indices) to capture multiple degrees of word similarity and linear regularities (e.g., vector arithmetic like king−man+woman≈queen), surveys prior neural language model work, and sets the goal of scalable, high‑quality word vectors from very large datasets and vocabularies.', methods_summary='They propose two computationally cheap log‑linear models—CBOW (predict target word from averaged context vectors) and Skip‑gram (predict context words from target word)—use hierarchical softmax with Huffman trees and parallel training (DistBelief/Adagrad), and analyze training complexity and hyperparameters (vector dimensionality, context size, epochs).', results_summary='Using a new Semantic‑Syntactic evaluation (≈8.9k semantic, 10.7k syntactic questions) and large corpora (up to Google News 6B tokens), Skip‑gram yields strong semantic accuracy while CBOW is faster and good on syntactic tasks; both outperform prior embeddings, scale to high dimensions and data (e.g., 1000‑d vectors trained in days with distributed training), and combining embeddings with RNNLMs improves downstream tasks (MSRC sentence completion SOTA).', discussion_summary='Simple, scalable architectures (CBOW/Skip‑gram) produce high‑quality, linearly regular word vectors that can be trained on orders‑of‑magnitude larger corpora than prior work, are useful across NLP applications, have been released (word2vec) and can be further scaled and improved in future work.')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_paper_data(paper_path: str):\n",
    "    paper_raw_text = load_pdf_text(paper_path)\n",
    "\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=f\"extract the information from the paper: {paper_raw_text}\",\n",
    "        text_format=PaperData\n",
    "    )\n",
    "    \n",
    "    output_parsed = response.output_parsed\n",
    "\n",
    "    print(output_parsed)\n",
    "    \n",
    "    upsert_paper_to_csv(output_parsed)\n",
    "    \n",
    "    return output_parsed\n",
    "\n",
    "extract_paper_data(\"paper3.pdf\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
